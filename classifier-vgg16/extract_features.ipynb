{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code adapted from https://gogul09.github.io/software/flower-recognition-deep-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "# keras imports\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.applications.vgg19 import VGG19, preprocess_input\n",
    "from keras.applications.xception import Xception, preprocess_input\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, load_model\n",
    "from keras.models import model_from_json\n",
    "\n",
    "# other imports\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import glob\n",
    "import cv2\n",
    "import h5py\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import datetime\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "\n",
    "import keras \n",
    "tbCallBack = keras.callbacks.TensorBoard(log_dir='H:/output/inceptionv3/Logs', write_graph=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config variables\n",
    "#model_name = config[\"model\"]\n",
    "#weights = config[\"weights\"]\n",
    "#include_top = config[\"include_top\"]\n",
    "#train_path = config[\"train_path\"]\n",
    "#features_path = config[\"features_path\"]\n",
    "#labels_path = config[\"labels_path\"]\n",
    "#test_size = config[\"test_size\"]\n",
    "#results = config[\"results\"]\n",
    "#model_path = config[\"model_path\"]\n",
    "#weights_path = config[\"weights_path\"]\n",
    "\n",
    "model_name = \"vgg16\"\n",
    "weights = \"imagenet\"\n",
    "include_top = False\n",
    "\n",
    "train_path= \"H:/Output2/training\"\n",
    "val_path= \"H:/Output2/test\"\n",
    "\n",
    "features_path= \"H:/Output2/meta/features.h5\"\n",
    "Labels_path = \"H:/Output2/meta/labels.h5\"\n",
    "\n",
    "Val_features_path= \"H:/Output2/meta/valfeatures.h5\"\n",
    "Val_Labels_path = \"H:/Output2/meta/vallabels.h5\"\n",
    "\n",
    "results = \"H:/Output2/meta/results.txt\"\n",
    "classifier_path = \"classifier.cpickle\"\n",
    "model_path = \"H:/Output2/meta/model\"\n",
    "weights_path = \"H:/Output2/meta/weight\"\n",
    "\n",
    "test_size= 0.20\n",
    "seed = 9\n",
    "num_classes = 101\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] successfully loaded base model and model...\n",
      "Wall time: 3.33 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\trabalho\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"fc...)`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create the pretrained models\n",
    "# check for pretrained weight usage or not\n",
    "# check for top layers to be included or not\n",
    "if model_name == \"vgg16\":\n",
    "    base_model = VGG16(weights=weights)\n",
    "    model = Model(input=base_model.input, output=base_model.get_layer('fc1').output)\n",
    "    image_size = (224, 224)\n",
    "elif model_name == \"vgg19\":\n",
    "    base_model = VGG19(weights=weights)\n",
    "    model = Model(input=base_model.input, output=base_model.get_layer('fc1').output)\n",
    "    image_size = (224, 224)\n",
    "elif model_name == \"resnet50\":\n",
    "    base_model = ResNet50(weights=weights)\n",
    "    model = Model(input=base_model.input, output=base_model.get_layer('flatten').output)\n",
    "    image_size = (224, 224)\n",
    "elif model_name == \"inceptionv3\":\n",
    "    base_model = InceptionV3(weights=weights)\n",
    "    model = Model(input=base_model.input, output=base_model.get_layer('avg_pool').output)\n",
    "    image_size = (299, 299)\n",
    "elif model_name == \"xception\":\n",
    "    base_model = Xception(weights=weights)\n",
    "    model = Model(input=base_model.input, output=base_model.get_layer('avg_pool').output)\n",
    "    image_size = (299, 299)\n",
    "else:\n",
    "    base_model = None\n",
    "\n",
    "print (\"[INFO] successfully loaded base model and model...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] encoding labels...\n",
      "Wall time: 3 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# path to training dataset\n",
    "train_labels = os.listdir(train_path)\n",
    "\n",
    "# encode the labels\n",
    "print(\"[INFO] encoding labels...\")\n",
    "le = LabelEncoder()\n",
    "le.fit([tl for tl in train_labels])\n",
    "\n",
    "# variables to hold features and labels\n",
    "features = []\n",
    "labels   = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheesecake', 'cheese_plate', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon', 'guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']\n"
     ]
    }
   ],
   "source": [
    "train_labels = os.listdir(train_path)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] processed  0 0\n",
      "[INFO] processed  0 1\n",
      "[INFO] processed  0 2\n",
      "[INFO] processed  0 3\n",
      "[INFO] processed  0 4\n",
      "[INFO] processed  0 5\n",
      "[INFO] processed  0 6\n",
      "[INFO] processed  0 7\n",
      "[INFO] processed  0 8\n",
      "[INFO] processed  0 9\n",
      "[INFO] processed  0 10\n",
      "[INFO] processed  0 11\n",
      "[INFO] processed  0 12\n",
      "[INFO] processed  0 13\n",
      "[INFO] processed  0 14\n",
      "[INFO] processed  0 15\n",
      "[INFO] processed  0 16\n",
      "[INFO] processed  0 17\n",
      "[INFO] processed  0 18\n",
      "[INFO] processed  0 19\n",
      "[INFO] processed  0 20\n",
      "[INFO] processed  0 21\n",
      "[INFO] processed  0 22\n",
      "[INFO] processed  0 23\n",
      "[INFO] processed  0 24\n",
      "[INFO] processed  0 25\n",
      "[INFO] processed  0 26\n",
      "[INFO] processed  0 27\n",
      "[INFO] processed  0 28\n",
      "[INFO] processed  0 29\n",
      "[INFO] processed  0 30\n",
      "[INFO] processed  0 31\n",
      "[INFO] processed  0 32\n",
      "[INFO] processed  0 33\n",
      "[INFO] processed  0 34\n",
      "[INFO] processed  0 35\n",
      "[INFO] processed  0 36\n",
      "[INFO] processed  0 37\n",
      "[INFO] processed  0 38\n",
      "[INFO] processed  0 39\n",
      "[INFO] processed  0 40\n",
      "[INFO] processed  0 41\n",
      "[INFO] processed  0 42\n",
      "[INFO] processed  0 43\n",
      "[INFO] processed  0 44\n",
      "[INFO] processed  0 45\n",
      "[INFO] processed  0 46\n",
      "[INFO] processed  0 47\n",
      "[INFO] processed  0 48\n",
      "[INFO] processed  0 49\n",
      "[INFO] processed  0 50\n",
      "[INFO] processed  0 51\n",
      "[INFO] processed  0 52\n",
      "[INFO] processed  0 53\n",
      "[INFO] processed  0 54\n",
      "[INFO] processed  0 55\n",
      "[INFO] processed  0 56\n",
      "[INFO] processed  0 57\n",
      "[INFO] processed  0 58\n",
      "[INFO] processed  0 59\n",
      "[INFO] processed  0 60\n",
      "[INFO] processed  0 61\n",
      "[INFO] processed  0 62\n",
      "[INFO] processed  0 63\n",
      "[INFO] processed  0 64\n",
      "[INFO] processed  0 65\n",
      "[INFO] processed  0 66\n",
      "[INFO] processed  0 67\n",
      "[INFO] processed  0 68\n",
      "[INFO] processed  0 69\n",
      "[INFO] processed  0 70\n",
      "[INFO] processed  0 71\n",
      "[INFO] processed  0 72\n",
      "[INFO] processed  0 73\n",
      "[INFO] processed  0 74\n",
      "[INFO] processed  0 75\n",
      "[INFO] processed  0 76\n",
      "[INFO] processed  0 77\n",
      "[INFO] processed  0 78\n",
      "[INFO] processed  0 79\n",
      "[INFO] processed  0 80\n",
      "[INFO] processed  0 81\n",
      "[INFO] processed  0 82\n",
      "[INFO] processed  0 83\n",
      "[INFO] processed  0 84\n",
      "[INFO] processed  0 85\n",
      "[INFO] processed  0 86\n",
      "[INFO] processed  0 87\n",
      "[INFO] processed  0 88\n",
      "[INFO] processed  0 89\n",
      "[INFO] processed  0 90\n",
      "[INFO] processed  0 91\n",
      "[INFO] processed  0 92\n",
      "[INFO] processed  0 93\n",
      "[INFO] processed  0 94\n",
      "[INFO] processed  0 95\n",
      "[INFO] processed  0 96\n",
      "[INFO] processed  0 97\n",
      "[INFO] processed  0 98\n",
      "[INFO] processed  0 99\n",
      "[INFO] processed  0 100\n",
      "[INFO] processed  0 101\n",
      "[INFO] processed  0 102\n",
      "[INFO] processed  0 103\n",
      "[INFO] processed  0 104\n",
      "[INFO] processed  0 105\n",
      "[INFO] processed  0 106\n",
      "[INFO] processed  0 107\n",
      "[INFO] processed  0 108\n",
      "[INFO] processed  0 109\n",
      "[INFO] processed  0 110\n",
      "[INFO] processed  0 111\n",
      "[INFO] processed  0 112\n",
      "[INFO] processed  0 113\n",
      "[INFO] processed  0 114\n",
      "[INFO] processed  0 115\n",
      "[INFO] processed  0 116\n",
      "[INFO] processed  0 117\n",
      "[INFO] processed  0 118\n",
      "[INFO] processed  0 119\n",
      "[INFO] processed  0 120\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# loop over all the labels in the folder\n",
    "for i, label in enumerate(train_labels):\n",
    "    cur_path = train_path + \"/\" + label\n",
    "    a=0\n",
    "    for image_path in glob.glob(cur_path + \"/*.jpg\"):\n",
    "        img = image.load_img(image_path, target_size=image_size)\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        feature = model.predict(x)\n",
    "        flat = feature.flatten()\n",
    "        features.append(flat)\n",
    "        labels.append(label)\n",
    "        print (\"[INFO] processed \",i,a)\n",
    "        a=a+1\n",
    "    print (\"[INFO] completed label\",label)\n",
    "    \n",
    "\n",
    "# encode the labels using LabelEncoder\n",
    "targetNames = np.unique(labels)\n",
    "le = LabelEncoder()\n",
    "le_labels = le.fit_transform(labels)\n",
    "\n",
    "print (le_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] encoding labels...\n",
      "Wall time: 1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# path to training dataset\n",
    "val_labels = os.listdir(val_path)\n",
    "\n",
    "# encode the labels\n",
    "print(\"[INFO] encoding labels...\")\n",
    "val_le = LabelEncoder()\n",
    "val_le.fit([vl for vl in val_labels])\n",
    "\n",
    "# variables to hold features and labels\n",
    "valfeatures = []\n",
    "vallabels   = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image_size' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# loop over all the labels in the folder\n",
    "for i, label in enumerate(val_labels):\n",
    "    cur_path = val_path + \"/\" + label\n",
    "    a=0\n",
    "    for image_path in glob.glob(cur_path + \"/*.jpg\"):\n",
    "        img = image.load_img(image_path, target_size=image_size)\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "        valfeature = model.predict(x)\n",
    "        flat = valfeature.flatten()\n",
    "        valfeatures.append(flat)\n",
    "        vallabels.append(label)\n",
    "        print (\"[INFO] processed \",i,a)\n",
    "        a=a+1\n",
    "    print (\"[INFO] completed label\",label)\n",
    "    \n",
    "\n",
    "# encode the labels using LabelEncoder\n",
    "ValtargetNames = np.unique(vallabels)\n",
    "valle = LabelEncoder()\n",
    "valle_labels = valle.fit_transform(vallabels)\n",
    "\n",
    "print (valle_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'le_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-6c05c8943590>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# get the shape of training labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"[STATUS] training labels:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mle_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"[STATUS] training labels shape:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mle_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# save features and labels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'le_labels' is not defined"
     ]
    }
   ],
   "source": [
    "# get the shape of training labels\n",
    "print (\"[STATUS] training labels:\",le_labels)\n",
    "print (\"[STATUS] training labels shape:\",le_labels.shape)\n",
    "\n",
    "# save features and labels\n",
    "h5f_data = h5py.File(features_path, 'w')\n",
    "h5f_data.create_dataset('dataset_1', data=np.array(features))\n",
    "\n",
    "h5f_label = h5py.File(Labels_path, 'w')\n",
    "h5f_label.create_dataset('dataset_1', data=np.array(le_labels))\n",
    "\n",
    "h5f_data.close()\n",
    "h5f_label.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the shape of training labels\n",
    "print (\"[STATUS] training labels:\",valle_labels)\n",
    "print (\"[STATUS] training labels shape:\",valle_labels.shape)\n",
    "\n",
    "# save features and labels\n",
    "h5f_val_data = h5py.File(Val_features_path, 'w')\n",
    "h5f_val_data.create_dataset('dataset_1', data=np.array(valfeatures))\n",
    "\n",
    "h5f_val_label = h5py.File(Val_Labels_path, 'w')\n",
    "h5f_val_label.create_dataset('dataset_1', data=np.array(valle_labels))\n",
    "\n",
    "h5f_val_data.close()\n",
    "h5f_val_label.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# save model and weights\n",
    "model_json = model.to_json()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#with open(model_path + str(test_size) + \".json\", \"w\") as json_file:\n",
    "with open(\"H:/Output2/meta/model0.1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# save weights\n",
    "#model.save_weights(model_path + str(test_size) + \".h5\")\n",
    "model.save_weights(\"H:/output/inceptionv3/model0.1.h5\")\n",
    "print(\"[STATUS] saved model and weights to disk..\")\n",
    "\n",
    "print (\"[STATUS] features and labels saved..\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
